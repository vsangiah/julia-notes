
Notes and takeaways from MPAGS Julia HPC sessions



Module 1 :Hardware and software basics

Memory:
Permanent memory:
	HDDs(slower RW) have high latencies as opposed to SSDs(higher RW)
Main(Random access) Memory: 
	volatile - loses info after power off
	high speeds (order of magnitude greater than SSDs)
	very low latencies (10-20ns)

1Byte is the smallest unit of addressable memory: even if you want to represent bools whjich technically requires 1 bit for true false, you would need 8bits chunk to represent it in machine

Central processing unit:
	perform the bulk of calculations: logical/arithmetic and interface with all I/Os and memory storages
	They have multiple cores each of which is responsible for independent calculations
	CPU has local storage: L1, L2, L3 caches
	[Insert pic of von neumann architecture]
	Data and memory stored together in memory (registers{temp memory} in case of CPU) [memory address register, memory data register]
	Program counter register - stores the address of instruction
	bus - connecting highways to transport info to i/o or storage

Modern CPUI architectures:
x86 architecture - 32 bit wide instruction set
x86_64 architecture - 64 bit wide instruction set
ARM - stripped down moder version of x86

[Insert the image of labelled CPU surface]

How memory gets RW:

CPU has to send out an instruction to find the memory in RAM and bring it to the core: goes via the L1 , L2, L3 cache. If CPU finds that data in L1 or L2 itself, it just skips the  asking the upper levels.
Once the memory is brought from RAM and used by CPU core , it stays in L1 or L2 cache - optimisation technique. Trip to RAM is time consuming as opposed to doing everything within CPUs (eg. 1ns vs 10ns)
L1 L2 L3 - expensive to make as it has to be etched in the same dye - small capacity: 64kB L1, 512kB L2, 8MB L3 are typical numbers

[RAM]      [high capacity: GBs]
^    v slow
L3 Cache   [capacity: ] for every 2 cores
^    v fast
L2 Cache  [capacity: ] for every 2 cores
^    v fast 
L1 Cache per CPU core [capacity: ]
^    v fast
[CPU Core]

CPU has 4 to 16 cores that perform the calculations, GPU has 1000 cores


Software -how it's run?
OS software - 
	abstraction to human interface, its placement:  user <-> apps <-> OS <-> hardware
	manages hardware and software resources
	provides utilities like disk, networkk
	controls loading and scheduling of software tasks - agian layer of abstraction
	provides virtual memory chunks for processes
	HPC is deloped in Linus

Encoding:
	machines understand binary
	datatype is needed to interpret how these numbers are and how calculations/operations are performed on them

	Integers
	8 bits signed integer - 8th bit is used for sign => N = (-1)^[8th bit value] 2^7 + [7th bit value] 2^6  + ... [1st bit value] 2^0
	8bits unsigned integer - uses all the 8bits 
	
	Floating point numbers (IEEE754 is mostly used)
	Float 32: 1st bit is used for sign, 8bits are exponent, 23 bits are mantissa
	N = (-1)^[32th bit value]x( 1 +  m / 2^23 )x 2^( e - 127 )
		- fft: imagine how floating point numbers are getting added

	Boolean - true or false

	Characters - ASCII 
	
	
Compilation:
Need to convert HLL code to LLL code (assembly) which wiill be later converted to binary instructions to CPU machine
HLL to assembly is called compilation process

How julia compialtion works:


Compile time: 
The julia source code is parsed to an Abstract Syntax Tree - [ to get this AST, try Meta.parse to get first stage, @macroexpand to get the expanded tree which is the second stage of AST]
this parsed AST expanded gets converted to Intermediate Representation - [to get the IR,, try @code_lowered]
lowered IR (easy to read) will infer the data types - this partially is controlled in runtime usss Method Dispatch to get IR
IR -> SSA(IR) optimized - [to get this optimized representation, try @code_typed]
SSA(IR) translated to LLVM IR which then gets converted to native code - [to get this native code, try @code_native]

Run time:
The native code is then executed
While executing - it goes through function calls and there will be method dispatches which will also play a role in infering data types and corresponding operations in getting the IR (@code_lowered)


LLVM compiler infrastructure:
	resualble toolchain for all languages
	front-end to generate LLVM IR
	provides backends forany architectures x86, x86_64, ARM
	Julia, Rust, C++, Python use LLVM tech




# Example:

julia> f(x) = 3*x*x + 2*x + 1
julia> @code_lowered f(5)
CodeInfo(
1 ─ %1 = Main.:+
│   %2 = Main.:*
│   %3 = (%2)(3, x, x)
│   %4 = Main.:*
│   %5 = (%4)(2, x)
│   %6 = (%1)(%3, %5, 1)
└──      return %6
)

julia> f2(x) = 30*x*x + 20*x + 40
julia> @code_typed f2(2.0)
CodeInfo(
1 ─ %1 = Base.mul_float(30.0, x)::Float64
│   %2 = Base.mul_float(%1, x)::Float64
│   %3 = Base.mul_float(20.0, x)::Float64
│   %4 = Base.add_float(%2, %3)::Float64
│   %5 = Base.add_float(%4, 40.0)::Float64
└──      return %5
) => Float64


julia>  @code_llvm debuginfo=:none f(11)
; Function Signature: f(Int64)
; Function Attrs: uwtable
define i64 @julia_f_2689(i64 signext %"x::Int64") #0 {
top:
  %0 = mul i64 %"x::Int64", 30
  %1 = add i64 %0, 20
  %2 = mul i64 %1, %"x::Int64"
  %3 = add i64 %2, 20
  ret i64 %3
}

julia>  @code_native debuginfo=:none f(11)
        .text
        .file   "f"
        .globl  julia_f_2783                    # -- Begin function julia_f_2783
        .p2align        4, 0x90
        .type   julia_f_2783,@function
julia_f_2783:                           # @julia_f_2783
; Function Signature: f(Int64)
        .cfi_startproc
# %bb.0:                                # %top
        #DEBUG_VALUE: f:x <- $rcx
        push    rbp
        .cfi_def_cfa_offset 16
        .cfi_offset rbp, -16
        mov     rbp, rsp
        .cfi_def_cfa_register rbp
        mov     rax, rcx
        shl     rax, 5
        sub     rax, rcx
        sub     rax, rcx
        add     rax, 20
        imul    rax, rcx
        add     rax, 20
        pop     rbp
        ret
.Lfunc_end0:
        .size   julia_f_2783, .Lfunc_end0-julia_f_2783
        .cfi_endproc
                                        # -- End function
        .section        ".note.GNU-stack","",@progbits

julia> @code_llvm debuginfo=:none f(2.0)+f(1)
; Function Signature: +(Float64, Int64)
; Function Attrs: uwtable
define double @"julia_+_3362"(double %"x::Float64", i64 signext %"y::Int64") #0 {
top:
  %0 = sitofp i64 %"y::Int64" to double
  %1 = fadd double %0, %"x::Float64"
  ret double %1
}







# Example 2# If there are constants in the code, they ll be evaluated before hand
julia> function mysum()
           s=0
           for i = 1:10000
               s+=i
                   end
           return s
       end
mysum (generic function with 1 method)

julia> @code_llvm debuginfo=:none mysum()
; Function Signature: mysum()
; Function Attrs: uwtable
define i64 @julia_mysum_3979() #0 {
top:
  ret i64 50005000
}


# Example 3 divide vs multiply

 function g(x::Float64)
       for i=1:1000
           x/=7
           end
           x
           end
g (generic function with 1 method)

julia> @btime g(1.0)
  2.800 μs (0 allocations: 0 bytes)
0.0

julia> function g(x::Float64)
       for i=1:1000
           x*=0.14285714285714285
           end
           x
           end
g (generic function with 1 method)

julia> @btime g(1.0)
  643.976 ns (0 allocations: 0 bytes)
0.0

!! This is 4.3 times fast


# SIMD - Single Instruction Multiple Data

Modern cpu have wide registers (temporary memory storages) - typically 256bit (with xeon,epyc processors) wide which can pack 4 x 64 bit values or 8 x 32 bit values in a single operations
ALU has special circuits to process these packs of data in the registers
This is parallelism at the hardware level perpective

example:
[x1;x2] + [y1;y2] = [x1+y1; x2+y2] happens in a single clock cycle:
addition of the numbers which are independent and storing of the result happens in one clock cycle

There are instruction sets (ISAs):
	SSE - Streaming SIMD Extensions supported in x86, x86_64 some are not supported for ARM
	Advanced Vector Extensions (AVX) - an advanced version of SSE eg AVX512 for large vectors
	Usually both these are included in workstation porcessors like Xeon, Epyc, Threadripper etc
	Notebook PC processors might only have SSE but not AVX

For your Linux machine:
 From a terminal, run “cat /proc/cpuinfo”. “sse2” will be listed as one of the “flags” if SSE2 is available.
Or for your Windows machine download CPUZ - https://www.cpuid.com/softwares/cpu-z.html

# Normal code:
function SUM_NORMAL(arr)
	sum = 0;
	for i in arr
		sum+=i
	end
	sum
end


# Add vector instructions to the for loop by using @simd macro (MACRO is meta programming construct - program to write more program)
	- Write @simd in front of for loops to promise that the iterations are independent and may be reordered. 

function SUM_SIMD(N)
	sum=0;
	@simd for i in arr
		sum+=i
	end
	sum
end

# Floating point arithmetic is not associative: (a+b)+c != a+(b+c)
So you'll get slightly differernt answers

# Benchmark results
julia> @btime SUM_NORMAL(rnum)
  57.520 ns

julia> @btime SUM_SIMD(rnum)
  38.710 ns

There will be better differnce when there is a better support for ISAs maybe.. (try this with any super computer)

Floating-point operations on reduction variables can be reordered or contracted, possibly causing different results than without @simd

Incorrect use of the @simd macro may cause unexpected results.



## Stack and Heap - lower level detail

	stack is a data structure - typically linear - access order is definitive (take the items only from the top ie LIFO)
	Demo of stack: adding a and b could be done by infix, prefix, postfix:
		a+b
		+(a,b)
		(a,b)+
	Usually computers use postfix to simplify their execution order
	eg: 5x(x+1)-1 is evaulated using this below
	POSTFIX notation: 1 5 x 2 x + * * - (dont need brackets)
	Imagine CPU has ALU + R1 R2 registers and sidebyside with a memory stack
	First populate the memory stack from left to right - only numbers coming so far
	Encounter a operator: pop 2 items to the two registers and evaluate the expression and store in R1
	
	push the value in R1 to the memory stack 
	R2 will get overwritten

 	repeat the process

	Stack is mainly used for function calls via stack frames (imagine piled up in memory stack) where eventually the result comes up in the top.


Advantages of using stack:
	all the intermediate variables %1 %2 ... etc are local variables - we dont need to keep track of them when the stack memory is not used.
	we can just move the stack pointer to next and forget the previous state.
	

